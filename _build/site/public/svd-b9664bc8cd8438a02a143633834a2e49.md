# Singular-Value Decomposition

Another decomposition idea, is the Singular-Value Decomposition (SVD).

$$
    \mathbf{A} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^T.
$$

This is the "singular-value decomposition", SVD. MATLAB will do it with \verb|[U,S,V] = svd(A)|.
:::{aside}
In the following, let's say the matrix $\mathbf{U}$ is an $m\times m$ orthogonal matrix, $\mathbf{V}$ is an $n\times n$ orthogonal matrix, and $\boldsymbol{\Sigma}$ is an $m\times n$ diagonal matrix, with positive values on the diagonal, decreasing in size: $s_1\geq s_2 \geq s_3 \geq ... \geq 0$. The number of "singular values" in this matrix is the lesser of $m$ and $n$, and for a square matrix, these values are the squares of the matrix's eigenvalues.
:::

## How to do the EVD in MATLAB/Python/Julia

Let's talk about how to implement these in your programming language first. I just grabbed the same matrix as for EVD.

:::{important}
The SVD can be applied to non-square matrices.
:::

::::{tab-set}
:::{tab-item} MATLAB
```matlab
M=[ [  0,   0,   1, 1/2];
    [1/3,   0,   0,   0];
    [1/3, 1/2,   0, 1/2];
    [1/3, 1/2,   0,   0]
    ];

[U,S,V] = svd(M);

% turns the diagonal of L into a column vector
S_vals=diag(S);

% reconstruct M
M_reconstructed=V*L*inv(V);
```
:::


:::{tab-item} Python
```python
import numpy as np

M = np.array([
    [0,   0,   1, 1/2],
    [1/3, 0,   0,   0],
    [1/3, 1/2, 0, 1/2],
    [1/3, 1/2, 0,   0]
])

# eigenvalues (L_vals) and eigenvectors (V)
L_vals, V = np.linalg.eig(M)

# to get the diagonal matrix L, if needed
L = np.diag(L_vals)

# reconstruct M
M_reconstructed = V @ L @ np.linalg.inv(V)
```
:::


:::{tab-item} Julia
```julia
using LinearAlgebra

M = [ 0    0    1  1/2;
     1/3   0    0    0;
     1/3  1/2   0  1/2;
     1/3  1/2   0    0 ]

results = eigen(M)

V = results.vectors
L = Diagonal(results.values)

M_reconstructed = V * L * inv(V)
```
:::
::::




## Low-Rank Approximation
If I'm being honest, I am not sure the practicalities of how the SVD is calculated numerically. But we can still try to develop some intuition about what is going on, and talking about the matrix rank\footnote{Recall: The rank is the number of linearly independent columns or rows.} is one way to do that.

Suppose we think of the rows\footnote{Equivalently could talk columns.} of a matrix as vectors. In a matrix of rank one, all the rows will be a scalar multiple of one vector, e.g.:

$$
    \mathbf{A}\equiv
    \begin{bmatrix}
        a_1 & a_2 & a_3\\
        2 a_1 & 2 a_2 & 2 a_3\\
        6a_1 & 6 a_2 & 6 a_3\\
    \end{bmatrix}=
    \begin{bmatrix}
        1\\2\\7
    \end{bmatrix}
    \begin{bmatrix}
        a_1 & a_2 & a_3
    \end{bmatrix}
$$

That is, the matrix can be written as $\mathbf{A}=\boldsymbol{\alpha}\mathbf{a}^T$, where $\boldsymbol{\alpha}$ and $\mathbf{a}$ are column vectors. In a matrix with rank two, all the rows will be scalar multiples of two vectors, i.e., two of the rows are linearly independent, and then the remaining rows are made up of linear combinations of those two rows, e.g.:

$$
    \mathbf{A}\equiv
    \begin{bmatrix}
        ~ ― & \mathbf{a} & ― ~\\
        ~ ―& \mathbf{b} & ― ~\\
        ~ ―& 3\mathbf{a}+4\mathbf{b} & ― ~
    \end{bmatrix}=
    \begin{bmatrix}
        1\\0\\4
    \end{bmatrix}
    \mathbf{a}^T+
        \begin{bmatrix}
        0\\1\\3
    \end{bmatrix}
    \mathbf{b}^T.
$$

This system could be written as $\mathbf{A}=\boldsymbol{\alpha}\mathbf{a}^T+\boldsymbol{\beta}\mathbf{b}^T$. Another way to put it: a matrix of rank two can be written as the sum of two rank one matrices.\footnote{We should include the caveat: ``...and cannot be represented as a rank one matrix.''} A matrix with rank three would require three rank one matrices, $\mathbf{A}=\boldsymbol{\alpha}\mathbf{a}^T+\boldsymbol{\beta}\mathbf{b}^T+\boldsymbol{\gamma}\mathbf{c}^T$, four takes four, five takes five, etc.

With one last gimmick -- write all the vectors as unit vectors multiplied by a scalar constant -- we can represent a matrix with rank $k$ as:

$$
    \mathbf{A}=\sum\limits_{i=1}^k s_i \mathbf{u}_i\mathbf{v}^T_i
$$

where $\mathbf{u}_i$ and $\mathbf{v}_i$ are unit vectors and $s_i$ is a scalar.\footnote{Relating to before: $\boldsymbol{\alpha}\mathbf{a}^T=s_1 \mathbf{u}_1\mathbf{v}_1^T$. This is generic and totally legal.} Not coincidentally, this looks like the SVD decomposition, and $s_i$ are the singular values.


## Applications
Many practical applications of SVD boil down to this: suppose we were to write a matrix $\mathbf{A}$ with $m$ rows and $n$ columns as
:::{aside}
The rank of the matrix is the smaller of $m$ and $n$, here I assumed it would be $m$.
:::

$$
    \mathbf{A}=s_1 \mathbf{u}_1\mathbf{v}^T_1 +
    s_2 \mathbf{u}_2\mathbf{v}^T_2 +
    s_3 \mathbf{u}_3\mathbf{v}^T_3 + \dots
$$

where $s_1 \geq s_2 \geq s_3 \geq ... \geq s_{m-1} \geq s_m \geq 0$. So we can say that the matrix has rank $m$, but let's further suppose that a certain number of the singular values are approximately zero, i.e., $s_m \approx 0$, $s_{m-1} \approx 0$, ... . These terms are not going to appreciably contribute to the summation to get $\mathbf{A}$. So the "full" matrix $\mathbf{A}$, with rank $m$, can be well-approximated by another matrix $\mathbf{A}_k$, with rank $k<m$. That is, the matrix $\mathbf{A}_k$ is the "low-rank approximation" of the "full" matrix $\mathbf{A}$.

### Image Processing/Data Compression
This rationalization lends itself well to understanding why SVD works in image processing. A black-and-white image can be interpreted as a matrix, with the value in each entry the intensity of that pixel.\footnote{We could think of the intensity as a float spanning $0\rightarrow 1$, but MATLAB uses integers between $0\rightarrow 255$.} To simplify the discussion, let's say the image/matrix is square, with $m$ rows and $m$ columns. So the matrix rank is $m$, and there are $m^2$ numbers stored in the matrix. The decomposition to $\mathbf{A}=\boldsymbol{\alpha}\mathbf{a}^T+\boldsymbol{\beta}\mathbf{b}^T+\boldsymbol{\gamma}\mathbf{c}^T+ \dots$ requires storage of $m(m+m)=2m^2$ numbers. So the SVD is a less-efficient way to store data?! Well, we can can get a really decent image with the low-rank approximation, wherein we essentially discard a bunch of information, thus reducing the storage size.

For example, on the next page I include a full image, with 3024 singular values, and the low-rank approximation of the image, keeping only 100 singular values. The second image, while inferior in quality, requires less than 6\% of the storage of the first.

```{figure} svd_LB1.png
:alt: 
:width: 550px
:align: center
```
```{figure} svd_LB2.png
:alt: 
:width: 550px
:align: center

Twilight picture (and made black-and-white) of my neighbors' cat, LB, in full detail (top); and in a low-rank approximation (bottom). The original image is $3024\times 4032$, meaning the matrix describing it has 12.2 million entries and 3024 singular values. The full SVD has $3024\times (3024+4032)=21.3$~million numbers. In the second image, we keep only 100 of the singular values, which means we need just $100\times (3024+4032)=706,000$ numbers to describe the image. We still get a lot of of detail of LB, of the wood, of the trees, the pine needles, etc. Comparing against the original size, that is ostensibly $<6\%$ the space -- the low-rank approximation does a really good job replicating the original image, with $\sim$1/20\textsuperscript{th} the space. (Full disclosure, the images I saved are 2.8~MB and 2.3~MB, signaling that this isn't how MATLAB actually saves the images.)
```


### Understanding Big Data Sets
We also may look to the singular-value decomposition for insight into otherwise unwieldy data. One example:\footnote{Taken from a textbook: \textit{Fundamentals of Numerical Computation} by Tobin Driscoll \& Richard Braun. Prof.~Driscoll explained some of the finer details to me in an email.} Put members of the Congress in rows in a matrix, and each bill's votes make up the columns. E.g., a congressperson who votes ``yea'', ``yea'', ``nay'' would have a row $[1,1,0]$. The SVD can be used to quantitatively assess each congressperson's partisanship.
:::{aside}
At which point we can bash them quantitatively instead of qualitatively.
:::

There are 435 members in the US Congress, and in a two-year session they vote on 600 bills or so, so this system would include $\sim 435\times 600=261,000$ data points. Definitely a lot of data, and it is not exactly obvious how to assess which of the 435 columns is most partisan.\footnote{Maybe something like: How often did you disagree with the median vote of your political party? But we'd need to additionally inject information about political party. And how do we deal with independents? ...We'd just have to ignore them.}

Big-picture, the system will likely be very well-described by a rank-2 matrix because the US is de-facto a two-party system where members of each party are "whipped" (there is literally a position within each party called the "whip") into voting along party lines. There will be enough alignment between any given republican and the stereotypical republican, and any given democrat and the stereotypical democrat, that the entire voting record can be described nearly perfectly with a linear combination of the array of R votes plus the array of D votes. Indeed, if each party votes as a block only the first two singular values are nonzero. (Assuming no third-party candidates/independents.)
:::{aside}
Good write-up on this aspect [here, by Cleve Moler](https://blogs.mathworks.com/cleve/2020/08/23/svd-quantifies-increasing-partisanship-in-the-u-s-senate/), quantitatively describing an increase in partisanship over time.
:::

We may also look closer to investigate individual members of Congress. Without getting too lost in the weeds, we can create a "partisanship score" for each congressperson with $\mathbf{A}\mathbf{v}_1$, i.e.~by multiplying the voting record matrix by the first column in $\mathbf{V}$; and a "bipartisanship score" by multiplying it by the second column. The outputs are vectors -- one entry per congressperson. The SVD thus gives us an unbiased score for each member that we can use to criticize them.
:::{aside}
A reasonable interpretation is that, similar to the dot product between two vectors telling us "the component of a vector along another vector", these multiplications tell us "the component of the matrix along the first and second right singular vectors." There would need to be an additional step wherein we decide how to normalize this information. Note also that $\mathbf{A}\mathbf{v}_1=\sigma_1 \mathbf{u}_1$, which is a representation that more explicitly includes the role of the singular values.
:::