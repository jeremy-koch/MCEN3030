# Using AI in school

**Author:** Jeremy Koch<sup>1,2</sup> \

## AI Misuse

Your instructors are not na√Øve. We are aware that AI use is widespread among students. That alone is not necessarily an issue, it is MISuse that is concerning, 


For engineering courses, the most audacious behavior I have seen is students taking screenshots of problems, pasting into ChatGPT, and then copying the results. These students possibly did not even read the problem! I have given the following (perhaps controversial) advice in the past, and I guess will give it here: If you care that little about mechanical engineering, you should consider switching majors. Find something you are actually passionate about. You are in the first stages of your careers and might spend the next 40 years as a mechanical engineer, it sounds like a [curse](https://en.wikipedia.org/wiki/The_Monkey%27s_Paw) to be given what you want today (very little effort on your homework), just for you to hate your job for the next four decades.

:::{tip}
If you would be ashamed to show your instructor your prompts, you are probably misusing AI.
:::

The thing that students need to realize is that understanding an answer is very different from being able to produce an answer. I suspect many students are asking LLMs for a solution, then they "study" it, and then think they "understand" the problem. That is basically never the case, and I think it explains why in some of my classes homework scores have crept upwards (perhaps 10 points higher on average than when I started this job) while exam scores have fallen (10+ points lower than when I started this job) -- the students have outsourced their homework to AI, . If it hasn't been made clear to you: the purpose of homework is not to finish it as quickly as possible, without learning anything.
:::{aside}
Not to be all "when I was your age, I had to walk to school uphill both ways, in the snow", but when I was an undergrad it was typical for homework to be worth 10% of the grade. I don't really believe that two or three time-constrained exams are a perfect indicator of your level of mastery of the content, but the widespread misuse of AI on homework has basically made it so that homework grades are meaningless. So we are likely trending back towards exam-heavy classes department-wide.
:::

If you are interested, you can read about [Bloom's Taxonomy](https://en.wikipedia.org/wiki/Bloom%27s_taxonomy), which is a framework for understanding how students master a topic. Studying LLM solutions may help you with "knowledge" and "comprehension", the lower levels of the taxonomy ("Ah that is how to format the inputs"). It will not help you get better at the "application", "analysis", "synthesis", or "evaluation" levels.


## AI Reasonable Use

It is my assessment that, overall, LLMs are plagiarism machines -- it's just that they go word-by-word, statistically, and so it is difficult to point to a single source from which their text was plagiarized. However, when it comes to coding, I have a comforting hypothesis that the LLMs are looking at code documentation (e.g. for [numpy piecewise](https://numpy.org/devdocs/reference/generated/numpy.piecewise.html)) and logicking together the inputs and outputs to achieve a goal. I feel a bit better about that, at least from an ethical standpoint.

This does not mean that LLMs are there to help you learn: if you want to learn, you must be thoughtful about how you interact with them. In particular: you must resist the urge to jump to the answer. So here are my recommendations on how to responsibly use AI, if your goal is to actually learn:
- Do not go to AI at the beginning. Do not go to AI immediately when you get stuck. Getting stuck and thinking about how to get unstuck is very much an authentic engineering experience and is probably the best way to learn. Reread the problem, think about what information is given, what information is missing that is needed, what assumptions could be made etc.
- If, after a while of being stuck, you have not made much progress, then talk to an AI. But write it in your own words, and try to be specific about the issue. "I am trying to write a root-solving algorithm using Newton-Raphson Method. I am stuck trying to figure out when to end the iterations. I suspect I should use a while statement but am confused on how to implement the logic." Honestly, I don't think the AI will need all that detail, but this is not about what the AI needs, it is about what you need! You need to get practice with using engineering language to define problems!
- If you encounter a cryptic error message, again: try to decipher it on your own, and if you can't figure it out, ask and be specific. Programming languages/environments are usually decent at explaining where the error occurs, giving a line number and the error type (ValueError, KeyError, etc.). "I get the following error: \<whatever the error is\>. It seems to be related to my function inputs but I am not sure what it means." That is not unreasonable, and again, you put some thought into it instead of just asking it to bail you out.
- Debugging via AI is a valid use, though I believe developing an eye for debugging is a very valuable skill. Try for a minute to debug on your own, but I get it: AI might save us from spending 90 minutes hunting for some tiny thing.

:::{warning}
The inputs to LLMs become part of its training data and thus that information more-or-less becomes public. Sometimes this is not the case -- e.g., CU seems to have an agreement with Google Gemini where our chats do not go to training their models. (I am not sure if students have the same access as faculty?) But I would in general be very careful about what you share. Indeed, in your career you may be prohibited from giving proprietary information to LLMs. You need to practice talking about the big ideas and then implementing the specifics locally, without giving away company secrets to LLMs!
:::