# EigenValue Decomposition

I want to introduce at least two other matrix decomposition techniques, the EigenValue Decomposition (EVD) and [Singular-Value Decomposition (SVD)](svd.md). These are interesting because they are a gateway to wisdom in engineering problems -- if, for example, you can identify which chemical reaction is associated with the largest positive eigenvalue, then you know a very interesting detail about how ignition is occurring and can potentially delay ignition by working to slow down that chemical reaction. Wisdom!
:::{seealso}
There are more decompositions out there! E.g., [the QR Decomposition](https://en.wikipedia.org/wiki/QR_decomposition) is relevant to model-fitting, which we will talk about shortly, but diving in to QR is not essential.
:::


For EVD and SVD, we are not going to program them ourselves, as we are doing with almost everything else in the course. But I thought it would be nice to have a few examples this semester of how to use built-in tools. 

## How to do the EVD in MATLAB/Python/Julia

Let's talk about how to implement these in your programming language first. I'll use the matrix associated with the PageRank algorithm, discussed below, as an example.
:::{important}
The EVD can only be applied to square matrices.
:::

::::{tab-set}
:::{tab-item} MATLAB
```matlab
M=[ [  0,   0,   1, 1/2];
    [1/3,   0,   0,   0];
    [1/3, 1/2,   0, 1/2];
    [1/3, 1/2,   0,   0]
    ];

% eigenvectors in V, and eigenvalues are the diagonal of L
[V,L]=eig(M);

% turns the diagonal of L into a column vector
L_vals=diag(L);

% reconstruct M
M_reconstructed=V*L*inv(V);
```
:::


:::{tab-item} Python
```python
import numpy as np

M = np.array([
    [0,   0,   1, 1/2],
    [1/3, 0,   0,   0],
    [1/3, 1/2, 0, 1/2],
    [1/3, 1/2, 0,   0]
])

# eigenvalues (L_vals) and eigenvectors (V)
L_vals, V = np.linalg.eig(M)

# to get the diagonal matrix L, if needed
L = np.diag(L_vals)

# reconstruct M
M_reconstructed = V @ L @ np.linalg.inv(V)
```
:::


:::{tab-item} Julia
```julia
using LinearAlgebra

M = [ 0    0    1  1/2;
     1/3   0    0    0;
     1/3  1/2   0  1/2;
     1/3  1/2   0    0 ]

results = eigen(M)

V = results.vectors
L = Diagonal(results.values)

M_reconstructed = V * L * inv(V)
```
:::
::::




## Eigenvectors and Values (this is largely covered in the video)
"The eigenvalue problem" involves finding, for a given matrix $\mathbf{A}$, the vectors $\mathbf{v}_i$ and corresponding scalars $\lambda_i$ such that

$$
    \mathbf{A}\mathbf{v}_i= \lambda_i\mathbf{v}_i.
$$

We say that the $\mathbf{v}_i$ are eigenvectors, and they have associated eigenvalues $\lambda_i$.

In your linear algebra course you probably spent some time calculating these things for a given matrix by creating a polynomial from the determinant: $\left|\mathbf{A}-\lambda \mathbf{I}\right|$ (where $\mathbf{I}$ is the identity matrix) and figuring out its roots. This is definitely NOT how eigenvalues are calculated for large systems. It is kind of a disservice that the typical linear algebra class had students finding eigenvalues of a $2\times 2$ matrix by hand from the polynomial. That whole exercise carries very little meaning, delivers no intuition, and is not even how eigenvalues are calculated in most practical situations. Instead, the eigenvalues are usually determined via an iterative process based on [QR Decomposition](https://en.wikipedia.org/wiki/QR_decomposition)... but that is too much math to talk about here.
:::{aside}
A $5\times 5$ matrix is not even "large", yet solving the fifth-degree polynomial is impossible for a human and not straightforward for a computer either. In many applications, the matrix in question is even larger, maybe much larger, and there is no hope of finding the polynomial's roots!
:::

## The Decomposition
An $n \times n$ matrix (of full rank) will have $n$ eigenvectors. If we gather them together as columns of a matrix (called $\mathbf{V}$) and include the eigenvalues in a diagonal matrix (called $\boldsymbol{\Lambda}$, "lambda", or $\boldsymbol{L}$... not the same as that in LU decomposition), we could write $\mathbf{A}\mathbf{V}=\mathbf{V}\boldsymbol{\Lambda}$. Or, writing in the typical decomposed form:
:::{aside}
We use the order $\mathbf{V}\boldsymbol{\Lambda}$ instead of $\boldsymbol{\Lambda}\mathbf{V}$ to make sure each column of $\mathbf{V}$ is multiplied by the correct $\lambda$.
:::


$$
    \mathbf{A}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{-1}.
$$

This is the EVD of matrix $\mathbf{A}$. The eigenvectors and eigenvalues are properties of $\mathbf{A}$, independent of $\mathbf{x}$ or $\mathbf{b}$.


## Applications
Thinking about what the eigenvalues mean for a system is really interesting. In many cases, the eigenvalue/vector pairs can be interpreted as the magnitudes and direction of greatest stretch (when applying a matrix to a vector), but that is not always a useful perspective. I'll give some examples here where that understanding is and is not meaningful.

### Linear Predator-Prey Models
Let's examine the discrete system

$$
    \begin{bmatrix}
        x_{i+1}\\
        y_{i+1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        2 & -1\\
        1 & -1
    \end{bmatrix}
    \begin{bmatrix}
        x_i\\
        y_i
    \end{bmatrix}.
$$

which is sometimes called a "linear predator-prey model" -- the idea is that the population of two species, often said to be foxes and rabbits, in year (or generation) $i+1$ is going to depend on the population in year/generation $i$. The matrix is interesting to create in itself, as it includes factors such as reproduction, food scarcity (not enough rabbits for the foxes to eat), etc.
:::{seealso}
In truth, this isn't a very meaningful predator-prey model -- those usually have a nonlinear term $\dot{x}\sim -xy$ that implies a consumption rate that increases with both the population of predator and prey. A nonlinear system can't immediately be put into a matrix framework, but can be linearized around critical points and then matrixified to understand those critical points. See the [Lotka-Volterra Equations](https://en.wikipedia.org/wiki/Lotkaâ€“Volterra_equations).
:::

The eigen-decomposition of this matrix is

$$
    \begin{bmatrix}
        2 & -1\\
        1 & -1
    \end{bmatrix}
    =
    \begin{bmatrix}
        0.9342 & 0.3568\\
        0.3568 & 0.9342
    \end{bmatrix}
    \begin{bmatrix}
        1.618 & 0\\
        0 & -0.618
    \end{bmatrix}
        \begin{bmatrix}
        1.2533 & -0.4787\\
        -0.4787 & 1.2533
    \end{bmatrix}.
$$

So, one positive eigenvalue, one negative eigenvalue. Starting with populations of $x_0=0.5$, $y_0=0.5$ (this is arbitrary), we can iterate through 1000 generations by repeatedly multiplying $[0.5,0.5]^T$ by the original matrix. The result is... well, a big value for each, but the more interesting thing is the ratio:

$$
    \frac{x_{1000}}{y_{1000}}=2.618=\frac{0.9342}{0.3568}.
$$

That is, the "vector" $[x,y]^T$ ends up pointing in the same direction as the eigenvector associated with the biggest eigenvalue. This provides some intuition about a meaning of the eigenvectors: when a matrix is applied to a vector, the vector will be stretched in the direction of the eigenvector with the greatest eigenvalue. Here, that has meaning: This ends up being the steady-state population ratio between species $x$ and $y$.
:::{note}
Again, a better population model would include some effect of overpopulation that would keep the populations from going to $\infty$.
:::




### A model with complex eigenvalues
What about the following?

$$
    \begin{bmatrix}
        x_{i+1}\\
        y_{i+1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        3 & 9\\
        -4 & -3
    \end{bmatrix}
    \begin{bmatrix}
        x\\
        y
    \end{bmatrix}
$$

This matrix decomposes into

$$
    \mathbf{V}=\begin{bmatrix}
        0.832 & 0.832\\
        -0.277+0.480i & -0.277-0.480i
    \end{bmatrix},
    \quad
    \boldsymbol{\Lambda}=\begin{bmatrix}
        5.196i & 0\\
        0 & -5.196i
    \end{bmatrix}.
$$

Complex eigenvectors and eigenvalues?
Starting again with $(x_0,y_0)=(0.5,0.5)$ and repeatedly applying the matrix via $\mathbf{x}_{i+1}=\mathbf{A}\mathbf{x}_i$ yields

$$
    \frac{x_{i+1}}{y_{i+1}}=1\text{ or }-1.714\text{, alternating back-and-forth.}
$$

Complex eigenvalues/vectors are associated with rotational motion. 
:::{aside}
For nonlinear systems, we analyze stationary/critical/fixed points. It is sometimes significant to the understanding of system dynamics to recognize that a point with purely imaginary eigenvalues will have trajectories that circle that stationary point -- no real components, purely imaginary, and so no movement away from or towards the fixed point.
:::

### Google's PageRank Algorithm
Let's imagine the internet has four websites on some topic. If we search for that topic, how is it decided which one is the first search result?

"If I am at website A, with links to websites B, C, and D, what is my likelihood of clicking a link to move to B, C, or D?" We can frame that as a matrix system with a simple assumption that you are 33\% likely to click on any of those links.

In the video, we did the following one: 

$$
\begin{bmatrix}
        0 & 0 & 1 & \frac{1}{2}\\
        \frac{1}{3} & 0 & 0 & 0\\
        \frac{1}{3} & \frac{1}{2} & 0 & \frac{1}{2}\\
        \frac{1}{3} & \frac{1}{2} & 0 & 0\\
    \end{bmatrix}
    \begin{bmatrix}
        x_\text{A}\\
        x_\text{B}\\
        x_\text{C}\\
        x_\text{D}
    \end{bmatrix}_1
    =
    \begin{bmatrix}
        x_\text{A}\\
        x_\text{B}\\
        x_\text{C}\\
        x_\text{D}
    \end{bmatrix}_2
$$

where the columns were interpreted as the fraction of links on a given site that led to another site -- thus, the zeros in the diagonal. A quick clarifying example: looking at the last column, if you are on website D and it has two links, one goes to website A and the other to C, so the chance of going to A is 50\% and C is 50\%.
:::{seealso}
I learned a lot of this from [Raluca Tanase & Remus Radu's](https://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/index.html) page. They address additional questions such as: What if there are two "subcultures" that reference a search term that don't reference each other at all? Then, depending on the starting population $\mathbf{x}$, a user might never find the second subculture. Page and Brin included the probability of a a random restart point that would then have a chance throw the searcher to the other sites.
:::



We can imagine an internet user marching their way through the links on various webpages, which would correspond to repeatedly multiplying the probability vector $\mathbf{x}$ by the matrix. And we might wonder: will this ever settle into a steady-state distribution, independent of the starting point, such that applying the matrix just gets us the same distribution again? I.e., $\mathbf{x}_i=\mathbf{x}_{i+1}$. Sounds like an eigenvalue problem -- the distribution we are describing would be an eigenvector with an associated eigenvalue of 1. For this example, the eigenvector associated with $\lambda=1$ is

$$
    \mathbf{x}=
    \begin{bmatrix}
        0.721\\0.240\\0.541\\0.361
    \end{bmatrix}.
$$

These are the page rankings, the order that Google would place these pages in their search results. Most to least important: A, C, D, B. This is not exactly how the algorithm works, particularly not in 2024, but this is the kernel of the idea.


The interesting thing is the amount of logic buried here. In this example, every node references C, so at first glance, C must be the most important, right? Well, C only references one node: A. That probably means A is really important! So we find C indeed has a large PageRank, but A's is larger.




### Chemical Mechanism Reduction
The logic from Google's PageRank Algorithm can be applied to an important modern topic: chemical mechanism reduction, which may be of interest to those who want to work with internal combustion engines.

The motivation: chemistry is vastly more complicated than a stoichiometric balance would have you believe. The "overall reaction" for methane (the simplest hydrocarbon) combustion $\text{CH}_4+2\text{O}_2\rightarrow \text{CO}_2+2\text{H}_2\text{O}$ actually involves many intermediate species -- most combustion chemists deal with a mechanism that 53 species and 325 reactions (such as H+O$_2\rightarrow$~O+OH, see the GRI Mechanism linked to the right). And that is just methane -- gasoline is judged by its octane rating, hinting that eight carbons are in the fuel molecule (though it is actually a complicated mixture of many hydrocarbon molecules); and diesel is judged on its cetane (hexadecane) rating, 16 carbons (though it is an even more complicated mixture). The detailed chemical mechanisms for iso-octane+air and hexadecane+air have about 1000 species each. In order to simulate these in a real engine, we'd need coupled ODEs for the concentration of each species at each time step, at each location, with turbulent flow, droplet vaporization physics, and molecular diffusivity. Good luck.
:::{aside}
A partial list of additional species in methane combustion: H, O, OH, HO2, H2O2, CH3, CH2, CH, C, HCO, CH2O, CO, C2H6 (yes, the molecules can get bigger), C2H2, .... andif we are worried about NOx pollution, we need to include N2, NO, NO2, NO3, NCO, CN, HCNO, ... . See [the GRI Mechanism](http://combustion.berkeley.edu/gri-mech/version30/text30.html).
:::

A strategy adopted by combustion/engine researchers: Use the PageRank algorithm to identify the most significant species to the reaction for given initial concentrations, given temperature, given pressure, etc., and toss out the insignificant ones. So a mechanism that contains 150 species and 750 reactions could be reduced to, say, 15 species and 75 reactions. That is 135 species that are dropped from the coupled ODE system and 135 species where we don't need to track their dispersal through flow and diffusion. A huge reduction in computational complexity.
:::{aside}
Depending on the thresholds chosen. Most turbulent flow simulations would have a hard time with even 15 species.
:::

### One More Idea in Chemistry: Computational Singular Perturbation
This is another combustion-related application that can help to reduce computational complexity. If we represent the concentration of a series of species with a vector $\mathbf{y}$ (i.e., the first entry might be number of moles of H$_2$, the second might be the number of moles of O$_2$,...), the change in those concentrations will be given by the set of coupled nonlinear differential equations
:::{aside}
Revisited my old advisor's textbook to write this and the previous section: Chung K. Law *Combustion Physics*.
:::

$$
    \frac{d\mathbf{y}}{dt} = \mathbf{g}(\mathbf{y})
$$

and then the time derivative of the right-hand-side will be given by 

$$
    \frac{d\mathbf{g}}{dt}=\frac{d\mathbf{g}}{d\mathbf{y}}\cdot\frac{d\mathbf{y}}{dt} = \frac{d\mathbf{g}}{d\mathbf{y}}\cdot\mathbf{g}=\mathbf{J}\cdot\mathbf{g}
$$

where $\mathbf{J}$ is the Jacobian matrix -- a matrix populated by derivatives of each equation, with respect to each independent variable.

There are more complications to this application than what I am about to say, particularly because $\mathbf{J}$ varies with time. But, basically: we can EVD $\mathbf{J}$ to reveal its eigenvectors, which are particular combinations of the reactions in the mechanism, and eigenvalues, which tell us something about how rapidly those reactions are proceeding. Namely, $\left|1/\lambda_i\right|$ is the characteristic time of the reaction "mode"/eigenvector. The "fast" eigenvectors can be removed from the coupled differential equations system and replaced with algebraic equations -- ostensibly, the associated reactions proceed infinitely quickly and immediately equilibrate -- which is another way to reduce the complexity of the calculation.